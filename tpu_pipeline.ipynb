{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTgknwPcKxBM",
        "outputId": "1645a37e-af18-427f-92a7-3d58ac9d83fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.8/441.8 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
        "                                 google-cloud-storage \\\n",
        "                                 google-cloud-pipeline-components \\\n",
        "                                 kfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE_0SBTjKyb-",
        "outputId": "c3ca66ba-13e0-4058-c102-60ab7359b80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "# Set project ID\n",
        "PROJECT_ID = \"***\"\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID} --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHb4Oom8LD0o"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8cxqG_DLG25",
        "outputId": "046f6e32-a9a4-4ba4-c1fe-92b49f555b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=1blrQ2KCpscC1hzx2CZB6isBHhxPKa&prompt=consent&access_type=offline&code_challenge=opY-gN-Xlp4HtnEo9v9HTfh8bfqcW5yBThbLPRpXvWs&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AeaYSHAYC6aZAUUCsSyzOz-nklJ27S_TDgskEt4WOKwQ2yjs6PNMscMpDG--2NO2w2Lsqg\n",
            "\n",
            "You are now logged in as [rashadha.rimsan@ncinga.net].\n",
            "Your current project is [data-science-projects-393906].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ],
      "source": [
        "# Authenticate with Google Cloud\n",
        "! gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEQo91NGLMAh"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtqG0HrQLWos"
      },
      "outputs": [],
      "source": [
        "# Create a bucket URI\n",
        "BUCKET_URI = f\"gs://bidirectional-lstm-model-{PROJECT_ID}-unique\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FKoPJI8Lca-",
        "outputId": "5c7de970-55d8-4dfd-9e46-2eaca8061afe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating gs://bidirectional-lstm-model-data-science-projects-393906-unique/...\n",
            "ServiceException: 409 A Cloud Storage bucket named 'bidirectional-lstm-model-data-science-projects-393906-unique' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
          ]
        }
      ],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuwPLFZBLea3"
      },
      "outputs": [],
      "source": [
        "# Set service account\n",
        "SERVICE_ACCOUNT = \"***\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hofRewLLrp1",
        "outputId": "d8d7a014-3e91-4f5b-ac50-934dfe517380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Service Account: 777232604101-compute@developer.gserviceaccount.com\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Use default service account if not provided\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"***\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    else:  # IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p7G5dI7Lvtu",
        "outputId": "426a650e-8f1d-4477-df0d-a03267632fd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No changes made to gs://bidirectional-lstm-model-data-science-projects-393906-unique/\n",
            "No changes made to gs://bidirectional-lstm-model-data-science-projects-393906-unique/\n"
          ]
        }
      ],
      "source": [
        "# Set IAM permissions for the service account\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_qGnB9kLxxd"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import google.cloud.aiplatform as aip\n",
        "import kfp\n",
        "from google_cloud_pipeline_components.types import artifact_types\n",
        "from google_cloud_pipeline_components.v1.custom_job.component import \\\n",
        "    custom_training_job as CustomTrainingJobOp\n",
        "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
        "                                                          ModelDeployOp)\n",
        "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
        "from kfp import compiler\n",
        "from kfp.dsl import importer_node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMYNV9yiL9Yl"
      },
      "outputs": [],
      "source": [
        "# Set pipeline root to save all model params\n",
        "PIPELINE_ROOT = \"{}/pipeline_root/tpu_bidirectional_pipeline\".format(BUCKET_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMY7R638MCbd"
      },
      "outputs": [],
      "source": [
        "# Initialize Vertex AI with staging bucket to store models artifacts\n",
        "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-scAnPe2MEFB"
      },
      "outputs": [],
      "source": [
        "# provides access to Google Cloud's AI Platform services\n",
        "from google.cloud.aiplatform import gapic\n",
        "\n",
        "# Define accelerator types for training and deployment\n",
        "# assigned values for the accelerator type and the number of accelerators to use during training\n",
        "TRAIN_TPU, TRAIN_NTPU = (\n",
        "    gapic.AcceleratorType.TPU_V2,\n",
        "    8,\n",
        ")  # Using TPU_V2 with 8 accelerators\n",
        "\n",
        "#  assigning values for the accelerator type and the number of accelerators to use during deployment\n",
        "DEPLOY_GPU, DEPLOY_NGPU = (gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pFvAxAkMFdX",
        "outputId": "c59ba9cb-fe0d-4846-d3f2-b24ddaa15d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deployment: us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-gpu.2-9:latest AcceleratorType.NVIDIA_TESLA_K80 1\n"
          ]
        }
      ],
      "source": [
        "# Define deployment details\n",
        "DEPLOY_VERSION = \"tf2-gpu.2-9\"\n",
        "\n",
        "# creating a deployment image URI based on the deployment version\n",
        "DEPLOY_IMAGE = \"us-docker.pkg.dev/cloud-aiplatform/prediction/{}:latest\".format(\n",
        "    DEPLOY_VERSION\n",
        ")\n",
        "\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZmTLp9FMHgi",
        "outputId": "cfd980b0-9ed0-4408-d621-c80403a730da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train machine type cloud-tpu\n",
            "Deploy machine type n1-standard-4\n"
          ]
        }
      ],
      "source": [
        "# Define compute types for training and deployment\n",
        "# define the machine type for training\n",
        "MACHINE_TYPE = \"cloud-tpu\"\n",
        "\n",
        "# TPU VMs do not require VCPU definition\n",
        "\n",
        "# the training will be done on TPU VMs\n",
        "TRAIN_COMPUTE = MACHINE_TYPE\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "# define the machine type for deployment\n",
        "MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\" #  using in the construction of the deployment compute type\n",
        "# the deployment will be on a machine with 4 virtual CPUs\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NTcuPI5MJSr",
        "outputId": "692ae852-9fa2-40b9-c475-ad80e9e2fee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['--epochs=20', '--steps=10000', '--distribute=tpu', '--bucket_name=a-tpu-training', '--train_data=BTCUSDT_ROOLING_1H-000000000001.csv'] gs://bidirectional-lstm-model-data-science-projects-393906-unique/pipeline_root/tpu_bidirectional_pipeline/model tpu_train_deploy\n"
          ]
        }
      ],
      "source": [
        "# Determine training strategy based on number of TPUs\n",
        "\n",
        "# if TRAIN_NTPU is not specified or if it's less than 2\n",
        "if not TRAIN_NTPU or TRAIN_NTPU < 2:\n",
        "    # GPU will be used for training\n",
        "    TRAIN_STRATEGY = \"single\"\n",
        "else:\n",
        "    TRAIN_STRATEGY = \"tpu\"\n",
        "\n",
        "EPOCHS = 20\n",
        "STEPS = 10000\n",
        "\n",
        "# Define trainer arguments\n",
        "TRAINER_ARGS = [\n",
        "    \"--epochs=\" + str(EPOCHS),\n",
        "    \"--steps=\" + str(STEPS),\n",
        "    \"--distribute=\" + TRAIN_STRATEGY,\n",
        "    \"--bucket_name=\" + \"a-tpu-training\",\n",
        "    \"--train_data=\" + \"BTCUSDT_ROOLING_1H-000000000001.csv\"\n",
        "]\n",
        "\n",
        "# create working dir to pass to job spec\n",
        "WORKING_DIR = f\"{PIPELINE_ROOT}/model\"\n",
        "\n",
        "MODEL_DISPLAY_NAME = \"tpu_train_deploy\"\n",
        "print(TRAINER_ARGS, WORKING_DIR, MODEL_DISPLAY_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY6Y-pQLq4T3",
        "outputId": "a6a419a1-896a-475d-b172-4f90476dc916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile Dockerfile\n",
        "# Specifies base image and tag\n",
        "FROM us-docker.pkg.dev/vertex-ai/training/tf-tpu-pod-base-cp38:latest\n",
        "WORKDIR /root\n",
        "\n",
        "# Download and install `tensorflow`.\n",
        "RUN pip install https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.12.0/tensorflow-2.12.0-cp38-cp38-linux_x86_64.whl\n",
        "\n",
        "RUN pip3 install gcsfs google-cloud-storage scikit-learn pandas numpy argparse\n",
        "\n",
        "# Download and install `libtpu`.\n",
        "# save `libtpu.so` in the '/lib' directory of the container image.\n",
        "# libtpu is a library for interfacing with TPUs\n",
        "RUN curl -L https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.6.0/libtpu.so -o /lib/libtpu.so\n",
        "\n",
        "# Copies the trainer code to the docker image.\n",
        "COPY train.py /root/train.py\n",
        "\n",
        "ENTRYPOINT [\"python3\", \"train.py\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCh_5WdyNdLO",
        "outputId": "ee390e6e-f6b6-4848-f4fe-43be03bb4cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train.py\n",
        "# Single, Mirror and Multi-Machine Distributed Training for tpu_bidirectional_pipeline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional, LayerNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--lr', dest='lr',\n",
        "                    default=0.01, type=float,\n",
        "                    help='Learning rate.')\n",
        "parser.add_argument('--epochs', dest='epochs',\n",
        "                    default=10, type=int,\n",
        "                    help='Number of epochs.')\n",
        "parser.add_argument('--steps', dest='steps',\n",
        "                    default=200, type=int,\n",
        "                    help='Number of steps per epoch.')\n",
        "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
        "                    help='distributed training strategy')\n",
        "parser.add_argument('--bucket_name', type=str, required=True,\n",
        "                    help='Bucket name on GCS')\n",
        "parser.add_argument('--train_data', type=str, required=True,\n",
        "                    help='Path to training data in GCS bucket')\n",
        "args = parser.parse_args()\n",
        "\n",
        "print('Python Version = {}'.format(sys.version))\n",
        "print('TensorFlow Version = {}'.format(tf.__version__))\n",
        "print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
        "print('DEVICES', device_lib.list_local_devices())\n",
        "\n",
        "# Single Machine, single compute device\n",
        "if args.distribute == 'single':\n",
        "    if tf.test.is_gpu_available():\n",
        "        #  training will be distributed across a single GPU\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "    else:\n",
        "        # training will be done on the CPU\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "# Single Machine, multiple TPU devices\n",
        "elif args.distribute == 'tpu':\n",
        "    # Connects to the TPU cluster and initializes the TPU system\n",
        "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
        "    tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "\n",
        "    # distribute training across multiple TPUs in the cluster\n",
        "    strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "# Single Machine, multiple compute device\n",
        "elif args.distribute == 'mirror':\n",
        "    # performs synchronous training across multiple GPUs on a single machine\n",
        "    # Each GPU will process a different batch of data\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "# Multiple Machine, multiple compute device\n",
        "elif args.distribute == 'multi':\n",
        "    # performs distributed synchronous training across multiple workers\n",
        "    # Each with potentially multiple GPUs\n",
        "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
        "\n",
        "# Multi-worker configuration\n",
        "print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "\n",
        "# Function to read data from GCS\n",
        "def read_data_from_gcs(bucket_name, train_data_path):\n",
        "    # Construct GCS path for the training data\n",
        "    gcs_input_path = f\"gs://{bucket_name}/{train_data_path}\"\n",
        "    # Read training data from GCS\n",
        "    return pd.read_csv(gcs_input_path)\n",
        "\n",
        "\n",
        "# Preparing dataset\n",
        "# Function to prepare the dataset\n",
        "def prepare_dataset(feature_array, sequence_length, sampling_rate, batch_size, target_scaler):\n",
        "    last = sequence_length * sampling_rate + sampling_rate\n",
        "    targets_normalized = target_scaler.fit_transform(feature_array[:-last][:, 6].reshape(-1, 1))\n",
        "    dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "        data=feature_array[sampling_rate:-sequence_length * sampling_rate],\n",
        "        targets=targets_normalized,\n",
        "        sequence_length=sequence_length,\n",
        "        sequence_stride=1,\n",
        "        sampling_rate=sampling_rate,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# Build the Keras model\n",
        "def build_and_compile_model():\n",
        "    # Model definition\n",
        "    model = Sequential([\n",
        "        LayerNormalization(axis=-1),  # First layer of the model\n",
        "        Bidirectional(LSTM(50, return_sequences=True), input_shape=(50, 12)),\n",
        "        Dropout(0.2),\n",
        "        Bidirectional(LSTM(50, return_sequences=True)),\n",
        "        Dropout(0.2),\n",
        "        Bidirectional(LSTM(50, return_sequences=True)),\n",
        "        Dropout(0.2),\n",
        "        Bidirectional(LSTM(25)),\n",
        "        Dropout(0.2),\n",
        "        Dense(5),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer, loss='mean_absolute_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
        "# Read data\n",
        "full_df = read_data_from_gcs(args.bucket_name, args.train_data)\n",
        "\n",
        "# Preprocess data\n",
        "feature_columns = full_df[[\"open\", \"high\", \"low\", \"close\", \"price_change_sign\", \"price_change\", \"price_change_percentage\", \"volume\", \"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\"]]\n",
        "feature_array = feature_columns.to_numpy()\n",
        "\n",
        "# Define target scaler\n",
        "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Adjust batch_size for optimization\n",
        "batch_size = 32\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = prepare_dataset(feature_array, 50, 60, batch_size, target_scaler)\n",
        "\n",
        "with strategy.scope():\n",
        "  # Creation of dataset, and model building/compiling need to be within\n",
        "  # `strategy.scope()`.\n",
        "  model = build_and_compile_model()\n",
        "\n",
        "model.fit(dataset, epochs=args.epochs, steps_per_epoch=args.steps)\n",
        "if args.distribute==\"tpu\":\n",
        "    # model will be saved locally on the TPU instance\n",
        "    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "    # model is saved to the specified directory\n",
        "    model.save(MODEL_DIR, options=save_locally)\n",
        "else:\n",
        "    model.save(MODEL_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97qLKtArNge2"
      },
      "outputs": [],
      "source": [
        "# Enable Artifact Registry service\n",
        "! gcloud services enable artifactregistry.googleapis.com\n",
        "\n",
        "# Update Google Cloud SDK components if in testing environment\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    ! sudo apt-get update --yes && sudo apt-get --only-upgrade --yes install google-cloud-sdk-cloud-run-proxy google-cloud-sdk-harbourbridge google-cloud-sdk-cbt google-cloud-sdk-gke-gcloud-auth-plugin google-cloud-sdk-kpt google-cloud-sdk-local-extract google-cloud-sdk-minikube google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-app-engine-python google-cloud-sdk-spanner-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-nomos google-cloud-sdk-package-go-module google-cloud-sdk-firestore-emulator kubectl google-cloud-sdk-datastore-emulator google-cloud-sdk-app-engine-python-extras google-cloud-sdk-cloud-build-local google-cloud-sdk-kubectl-oidc google-cloud-sdk-anthos-auth google-cloud-sdk-app-engine-grpc google-cloud-sdk-pubsub-emulator google-cloud-sdk-datalab google-cloud-sdk-skaffold google-cloud-sdk google-cloud-sdk-terraform-tools google-cloud-sdk-config-connector\n",
        "    ! gcloud components update --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n7j-U_tNjml",
        "outputId": "8898d5ca-4e51-45b2-dc70-3a3513a049e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "groupadd: group 'docker' already exists\n"
          ]
        }
      ],
      "source": [
        "# Create Docker group\n",
        "!sudo groupadd docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsPwdhGxNlJx",
        "outputId": "ed1699e6-4228-4589-ee42-bcbdb31f3ccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usage: usermod [options] LOGIN\n",
            "\n",
            "Options:\n",
            "  -b, --badnames                allow bad names\n",
            "  -c, --comment COMMENT         new value of the GECOS field\n",
            "  -d, --home HOME_DIR           new home directory for the user account\n",
            "  -e, --expiredate EXPIRE_DATE  set account expiration date to EXPIRE_DATE\n",
            "  -f, --inactive INACTIVE       set password inactive after expiration\n",
            "                                to INACTIVE\n",
            "  -g, --gid GROUP               force use GROUP as new primary group\n",
            "  -G, --groups GROUPS           new list of supplementary GROUPS\n",
            "  -a, --append                  append the user to the supplemental GROUPS\n",
            "                                mentioned by the -G option without removing\n",
            "                                the user from other groups\n",
            "  -h, --help                    display this help message and exit\n",
            "  -l, --login NEW_LOGIN         new value of the login name\n",
            "  -L, --lock                    lock the user account\n",
            "  -m, --move-home               move contents of the home directory to the\n",
            "                                new location (use only with -d)\n",
            "  -o, --non-unique              allow using duplicate (non-unique) UID\n",
            "  -p, --password PASSWORD       use encrypted password for the new password\n",
            "  -R, --root CHROOT_DIR         directory to chroot into\n",
            "  -P, --prefix PREFIX_DIR       prefix directory where are located the /etc/* files\n",
            "  -s, --shell SHELL             new login shell for the user account\n",
            "  -u, --uid UID                 new UID for the user account\n",
            "  -U, --unlock                  unlock the user account\n",
            "  -v, --add-subuids FIRST-LAST  add range of subordinate uids\n",
            "  -V, --del-subuids FIRST-LAST  remove range of subordinate uids\n",
            "  -w, --add-subgids FIRST-LAST  add range of subordinate gids\n",
            "  -W, --del-subgids FIRST-LAST  remove range of subordinate gids\n",
            "  -Z, --selinux-user SEUSER     new SELinux user mapping for the user account\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Add user to Docker group\n",
        "!sudo usermod -a -G docker ${USER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3cE-WCtNm5f"
      },
      "outputs": [],
      "source": [
        "# Define repository and image names\n",
        "REPOSITORY = \"tpu-training-repository\"\n",
        "IMAGE = \"tpu-train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5Cr6jQENwsK",
        "outputId": "fa6cecda-4764-4279-8870-d63900da03bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
          ]
        }
      ],
      "source": [
        "# Create Artifact Registry repository\n",
        "!gcloud artifacts repositories create $REPOSITORY --repository-format=docker \\\n",
        "--location=us-central1 --description=\"Vertex TPU training repository\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6b-cLCkN3bh"
      },
      "outputs": [],
      "source": [
        "# Define train image\n",
        "TRAIN_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}:latest\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPfSPmrHN6u-",
        "outputId": "6a37dd17-9046-403b-946c-54d24aef2003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "us-central1-docker.pkg.dev/data-science-projects-393906/tpu-training-repository/tpu-train:latest\n"
          ]
        }
      ],
      "source": [
        "print(TRAIN_IMAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1GsLTwyN-ah",
        "outputId": "92df42c8-bdad-48bd-dc47-4c9cef78375f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating temporary tarball archive of 63 file(s) totalling 57.4 MiB before compression.\n",
            "Uploading tarball of [.] to [gs://data-science-projects-393906_cloudbuild/source/1708017328.172113-726b8bc959e140c3bef4a0476f0732a1.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/data-science-projects-393906/locations/us-central1/builds/ae7353e8-e5f9-4bb8-993c-a92a94c78b5d].\n",
            "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/ae7353e8-e5f9-4bb8-993c-a92a94c78b5d?project=777232604101 ].\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"ae7353e8-e5f9-4bb8-993c-a92a94c78b5d\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://data-science-projects-393906_cloudbuild/source/1708017328.172113-726b8bc959e140c3bef4a0476f0732a1.tgz#1708017338642719\n",
            "Copying gs://data-science-projects-393906_cloudbuild/source/1708017328.172113-726b8bc959e140c3bef4a0476f0732a1.tgz#1708017338642719...\n",
            "/ [1 files][  6.8 MiB/  6.8 MiB]                                                \n",
            "Operation completed over 1 objects/6.8 MiB.\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon  60.25MB\n",
            "Step 1/7 : FROM us-docker.pkg.dev/vertex-ai/training/tf-tpu-pod-base-cp38:latest\n",
            "latest: Pulling from vertex-ai/training/tf-tpu-pod-base-cp38\n",
            "f2f58072e9ed: Pulling fs layer\n",
            "5c8cfbf51e6e: Pulling fs layer\n",
            "aa3a609d1579: Pulling fs layer\n",
            "094e7d9bb04e: Pulling fs layer\n",
            "2cbfd734f382: Pulling fs layer\n",
            "aa86ac293d0f: Pulling fs layer\n",
            "2d4889cd3d17: Pulling fs layer\n",
            "292b192e46c0: Pulling fs layer\n",
            "084f34f4cbc1: Pulling fs layer\n",
            "7027e5766605: Pulling fs layer\n",
            "a0be4c707c57: Pulling fs layer\n",
            "9aa55ec5ae9a: Pulling fs layer\n",
            "15928410aac3: Pulling fs layer\n",
            "094e7d9bb04e: Waiting\n",
            "2cbfd734f382: Waiting\n",
            "aa86ac293d0f: Waiting\n",
            "2d4889cd3d17: Waiting\n",
            "292b192e46c0: Waiting\n",
            "084f34f4cbc1: Waiting\n",
            "7027e5766605: Waiting\n",
            "a0be4c707c57: Waiting\n",
            "9aa55ec5ae9a: Waiting\n",
            "15928410aac3: Waiting\n",
            "5c8cfbf51e6e: Verifying Checksum\n",
            "5c8cfbf51e6e: Download complete\n",
            "aa3a609d1579: Verifying Checksum\n",
            "aa3a609d1579: Download complete\n",
            "f2f58072e9ed: Verifying Checksum\n",
            "f2f58072e9ed: Download complete\n",
            "094e7d9bb04e: Verifying Checksum\n",
            "094e7d9bb04e: Download complete\n",
            "aa86ac293d0f: Verifying Checksum\n",
            "aa86ac293d0f: Download complete\n",
            "292b192e46c0: Verifying Checksum\n",
            "292b192e46c0: Download complete\n",
            "084f34f4cbc1: Verifying Checksum\n",
            "084f34f4cbc1: Download complete\n",
            "2d4889cd3d17: Verifying Checksum\n",
            "2d4889cd3d17: Download complete\n",
            "a0be4c707c57: Verifying Checksum\n",
            "a0be4c707c57: Download complete\n",
            "9aa55ec5ae9a: Verifying Checksum\n",
            "9aa55ec5ae9a: Download complete\n",
            "7027e5766605: Verifying Checksum\n",
            "7027e5766605: Download complete\n",
            "15928410aac3: Verifying Checksum\n",
            "15928410aac3: Download complete\n",
            "2cbfd734f382: Verifying Checksum\n",
            "2cbfd734f382: Download complete\n",
            "f2f58072e9ed: Pull complete\n",
            "5c8cfbf51e6e: Pull complete\n",
            "aa3a609d1579: Pull complete\n",
            "094e7d9bb04e: Pull complete\n",
            "2cbfd734f382: Pull complete\n",
            "aa86ac293d0f: Pull complete\n",
            "2d4889cd3d17: Pull complete\n",
            "292b192e46c0: Pull complete\n",
            "084f34f4cbc1: Pull complete\n",
            "7027e5766605: Pull complete\n",
            "a0be4c707c57: Pull complete\n",
            "9aa55ec5ae9a: Pull complete\n",
            "15928410aac3: Pull complete\n",
            "Digest: sha256:ab10e3a11312f364ae4c65fe797c9b51947aa14c2eac6943b803e55b2794fbb1\n",
            "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/tf-tpu-pod-base-cp38:latest\n",
            " ---> 98396a0cebbe\n",
            "Step 2/7 : WORKDIR /root\n",
            " ---> Running in f057e1f752cc\n",
            "Removing intermediate container f057e1f752cc\n",
            " ---> cde13550198d\n",
            "Step 3/7 : RUN pip install https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.12.0/tensorflow-2.12.0-cp38-cp38-linux_x86_64.whl\n",
            " ---> Running in a913e850900b\n",
            "Collecting tensorflow==2.12.0\n",
            "  Downloading https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.12.0/tensorflow-2.12.0-cp38-cp38-linux_x86_64.whl (211.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.4/211.4 MB 8.7 MB/s eta 0:00:00\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.60.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 20.7 MB/s eta 0:00:00\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Collecting keras<2.13,>=2.12.0\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 79.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.16.0)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 KB 10.5 MB/s eta 0:00:00\n",
            "Collecting packaging\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 KB 8.6 MB/s eta 0:00:00\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting h5py>=2.9.0\n",
            "  Downloading h5py-3.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 77.8 MB/s eta 0:00:00\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 440.7/440.7 KB 47.2 MB/s eta 0:00:00\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 KB 9.9 MB/s eta 0:00:00\n",
            "Collecting tensorboard<2.13,>=2.12\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 58.0 MB/s eta 0:00:00\n",
            "Collecting numpy<1.24,>=1.22\n",
            "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 52.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (3.20.3)\n",
            "Collecting libclang>=13.0.0\n",
            "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.9/22.9 MB 46.7 MB/s eta 0:00:00\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 85.4 MB/s eta 0:00:00\n",
            "Collecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting absl-py>=1.0.0\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 KB 23.3 MB/s eta 0:00:00\n",
            "Collecting wrapt<1.15,>=1.11.0\n",
            "  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 KB 13.9 MB/s eta 0:00:00\n",
            "Collecting typing-extensions>=3.6.6\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (57.5.0)\n",
            "Collecting jax>=0.3.15\n",
            "  Downloading jax-0.4.13.tar.gz (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 73.4 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.38.4)\n",
            "Collecting importlib-metadata>=4.6\n",
            "  Downloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
            "Collecting ml-dtypes>=0.1.0\n",
            "  Downloading ml_dtypes-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 69.4 MB/s eta 0:00:00\n",
            "Collecting scipy>=1.7\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.5/34.5 MB 42.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 70.7 MB/s eta 0:00:00\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 KB 28.0 MB/s eta 0:00:00\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.9/103.9 KB 15.1 MB/s eta 0:00:00\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.22.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.26.16)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.3.0)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2023.7.22)\n",
            "Collecting MarkupSafe>=2.1.1\n",
            "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.5.0)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 KB 24.5 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (pyproject.toml): started\n",
            "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for jax: filename=jax-0.4.13-py3-none-any.whl size=1518703 sha256=41644f234c44d7b623008f213ee69dc63edf2b9ed308156b6accb565f50a5c9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/d9/15/d2800d4089dc4c77299ac7513c6aa1036f5491edbd2bf6ba16\n",
            "Successfully built jax\n",
            "Installing collected packages: libclang, flatbuffers, zipp, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, packaging, oauthlib, numpy, MarkupSafe, keras, grpcio, google-pasta, gast, astunparse, absl-py, werkzeug, scipy, requests-oauthlib, opt-einsum, ml-dtypes, importlib-metadata, h5py, markdown, jax, google-auth-oauthlib, tensorboard, tensorflow\n",
            "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.4.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.60.1 h5py-3.10.0 importlib-metadata-7.0.1 jax-0.4.13 keras-2.12.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.2 requests-oauthlib-1.3.1 scipy-1.10.1 tensorboard-2.12.3 tensorboard-data-server-0.7.2 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 typing-extensions-4.9.0 werkzeug-3.0.1 wrapt-1.14.1 zipp-3.17.0\n",
            "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
            "\u001b[0mRemoving intermediate container a913e850900b\n",
            " ---> 10cb779b902d\n",
            "Step 4/7 : RUN pip3 install gcsfs google-cloud-storage scikit-learn pandas numpy argparse\n",
            " ---> Running in ad9540894c75\n",
            "Collecting gcsfs\n",
            "  Downloading gcsfs-2024.2.0-py2.py3-none-any.whl (33 kB)\n",
            "Collecting google-cloud-storage\n",
            "  Downloading google_cloud_storage-2.14.0-py2.py3-none-any.whl (121 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 KB 2.1 MB/s eta 0:00:00\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.1/11.1 MB 46.7 MB/s eta 0:00:00\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/12.4 MB 45.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (1.23.5)\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting fsspec==2024.2.0\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.9/170.9 KB 25.1 MB/s eta 0:00:00\n",
            "Collecting decorator>4.1.2\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 77.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.8/site-packages (from gcsfs) (2.22.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from gcsfs) (2.31.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.8/site-packages (from gcsfs) (1.0.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/site-packages (from google-cloud-storage) (1.34.0)\n",
            "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
            "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Collecting google-resumable-media>=2.6.0\n",
            "  Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 KB 13.8 MB/s eta 0:00:00\n",
            "Collecting google-auth>=1.2\n",
            "  Downloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 186.8/186.8 KB 24.9 MB/s eta 0:00:00\n",
            "Collecting joblib>=1.1.1\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.2/302.2 KB 39.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
            "Collecting python-dateutil>=2.8.2\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 KB 34.6 MB/s eta 0:00:00\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 505.5/505.5 KB 48.1 MB/s eta 0:00:00\n",
            "Collecting tzdata>=2022.1\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.4/345.4 KB 42.6 MB/s eta 0:00:00\n",
            "Collecting async-timeout<5.0,>=4.0\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.9/240.9 KB 34.5 MB/s eta 0:00:00\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (308 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.8/308.8 KB 38.6 MB/s eta 0:00:00\n",
            "Collecting attrs>=17.3.0\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.8/60.8 KB 10.7 MB/s eta 0:00:00\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.3/129.3 KB 22.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.60.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.20.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/site-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/site-packages (from google-auth>=1.2->gcsfs) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/site-packages (from google-auth>=1.2->gcsfs) (0.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->gcsfs) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->gcsfs) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->gcsfs) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->gcsfs) (2023.7.22)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/site-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
            "Installing collected packages: pytz, argparse, tzdata, threadpoolctl, python-dateutil, multidict, joblib, google-crc32c, fsspec, frozenlist, decorator, attrs, async-timeout, yarl, scikit-learn, pandas, google-resumable-media, google-auth, aiosignal, aiohttp, google-cloud-core, google-cloud-storage, gcsfs\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.22.0\n",
            "    Uninstalling google-auth-2.22.0:\n",
            "      Successfully uninstalled google-auth-2.22.0\n",
            "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 argparse-1.4.0 async-timeout-4.0.3 attrs-23.2.0 decorator-5.1.1 frozenlist-1.4.1 fsspec-2024.2.0 gcsfs-2024.2.0 google-auth-2.27.0 google-cloud-core-2.4.1 google-cloud-storage-2.14.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 joblib-1.3.2 multidict-6.0.5 pandas-2.0.3 python-dateutil-2.8.2 pytz-2024.1 scikit-learn-1.3.2 threadpoolctl-3.3.0 tzdata-2024.1 yarl-1.9.4\n",
            "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
            "\u001b[0mRemoving intermediate container ad9540894c75\n",
            " ---> ab9cef847e13\n",
            "Step 5/7 : RUN curl -L https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.6.0/libtpu.so -o /lib/libtpu.so\n",
            " ---> Running in 400f61feb92d\n",
            "\u001b[91m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  189M  100  189M    0     0   106M      0  0:00:01  0:00:01 --:--:--  106M\n",
            "\u001b[0mRemoving intermediate container 400f61feb92d\n",
            " ---> 35208af03c8b\n",
            "Step 6/7 : COPY train.py /root/train.py\n",
            " ---> 25312bf14313\n",
            "Step 7/7 : ENTRYPOINT [\"python3\", \"train.py\"]\n",
            " ---> Running in a5737a52589d\n",
            "Removing intermediate container a5737a52589d\n",
            " ---> 35d98606d036\n",
            "Successfully built 35d98606d036\n",
            "Successfully tagged us-central1-docker.pkg.dev/data-science-projects-393906/tpu-training-repository/tpu-train:latest\n",
            "PUSH\n",
            "Pushing us-central1-docker.pkg.dev/data-science-projects-393906/tpu-training-repository/tpu-train:latest\n",
            "The push refers to repository [us-central1-docker.pkg.dev/data-science-projects-393906/tpu-training-repository/tpu-train]\n",
            "dd4d2a5f5170: Preparing\n",
            "27905ce6222b: Preparing\n",
            "9681b136c4f3: Preparing\n",
            "f67604c289b9: Preparing\n",
            "65a641ff212c: Preparing\n",
            "f16bea534ac9: Preparing\n",
            "1761a168d161: Preparing\n",
            "6ff411a679bf: Preparing\n",
            "69b8c938e1b0: Preparing\n",
            "c1b4f27bd6bc: Preparing\n",
            "33653abce00f: Preparing\n",
            "1cad4dc57058: Preparing\n",
            "4ff8844d474a: Preparing\n",
            "b77487480ddb: Preparing\n",
            "cd247c0fb37b: Preparing\n",
            "cfdd5c3bd77e: Preparing\n",
            "870a241bfebd: Preparing\n",
            "c1b4f27bd6bc: Waiting\n",
            "33653abce00f: Waiting\n",
            "1cad4dc57058: Waiting\n",
            "4ff8844d474a: Waiting\n",
            "b77487480ddb: Waiting\n",
            "cd247c0fb37b: Waiting\n",
            "cfdd5c3bd77e: Waiting\n",
            "870a241bfebd: Waiting\n",
            "f16bea534ac9: Waiting\n",
            "1761a168d161: Waiting\n",
            "6ff411a679bf: Waiting\n",
            "69b8c938e1b0: Waiting\n",
            "65a641ff212c: Layer already exists\n",
            "f16bea534ac9: Layer already exists\n",
            "1761a168d161: Layer already exists\n",
            "dd4d2a5f5170: Pushed\n",
            "6ff411a679bf: Layer already exists\n",
            "69b8c938e1b0: Layer already exists\n",
            "33653abce00f: Layer already exists\n",
            "c1b4f27bd6bc: Layer already exists\n",
            "4ff8844d474a: Layer already exists\n",
            "1cad4dc57058: Layer already exists\n",
            "b77487480ddb: Layer already exists\n",
            "cd247c0fb37b: Layer already exists\n",
            "cfdd5c3bd77e: Layer already exists\n",
            "870a241bfebd: Layer already exists\n",
            "27905ce6222b: Pushed\n",
            "9681b136c4f3: Pushed\n",
            "f67604c289b9: Pushed\n",
            "latest: digest: sha256:0e25f64c00f8c5b9545c1232ffb5eb78c1dde2fb2dc6bb5c0121be1a763ef500 size: 3897\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                                               STATUS\n",
            "ae7353e8-e5f9-4bb8-993c-a92a94c78b5d  2024-02-15T17:15:39+00:00  2M52S     gs://data-science-projects-393906_cloudbuild/source/1708017328.172113-726b8bc959e140c3bef4a0476f0732a1.tgz  us-central1-docker.pkg.dev/data-science-projects-393906/tpu-training-repository/tpu-train (+1 more)  SUCCESS\n"
          ]
        }
      ],
      "source": [
        "# Submit build to Google Cloud Build\n",
        "! gcloud builds submit --region=us-central1 --tag $TRAIN_IMAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "L5NGS_ubRN9U",
        "outputId": "42a30889-73e2-4861-bbc1-61bc052c0315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting functions-framework==3.*\n",
            "  Downloading functions_framework-3.5.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: flask<4.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from functions-framework==3.*) (2.2.5)\n",
            "Requirement already satisfied: click<9.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from functions-framework==3.*) (8.1.7)\n",
            "Collecting watchdog>=1.0.0 (from functions-framework==3.*)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudevents<2.0.0,>=1.2.0 (from functions-framework==3.*)\n",
            "  Downloading cloudevents-1.10.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gunicorn>=19.2.0 (from functions-framework==3.*)\n",
            "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecation<3.0,>=2.0 (from cloudevents<2.0.0,>=1.2.0->functions-framework==3.*)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask<4.0,>=1.0->functions-framework==3.*) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask<4.0,>=1.0->functions-framework==3.*) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask<4.0,>=1.0->functions-framework==3.*) (2.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn>=19.2.0->functions-framework==3.*) (23.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask<4.0,>=1.0->functions-framework==3.*) (2.1.5)\n",
            "Installing collected packages: watchdog, gunicorn, deprecation, cloudevents, functions-framework\n",
            "Successfully installed cloudevents-1.10.1 deprecation-2.1.0 functions-framework-3.5.0 gunicorn-21.2.0 watchdog-4.0.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "9662ea61e1f64a8f9713b043e2edc5e6",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "! pip install functions-framework==3.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqjB9xhcXwUJ"
      },
      "outputs": [],
      "source": [
        "import functions_framework\n",
        "from google.cloud import pubsub\n",
        "\n",
        "# Initialize Google Cloud Pub/Sub client\n",
        "pubsub_client = pubsub.SubscriberClient()\n",
        "\n",
        "# Define the subscription name\n",
        "subscription_name = \"projects/data-science-projects-393906/subscriptions/changeInStorageBucket-sub\"\n",
        "\n",
        "# Define a global variable to store the index\n",
        "index_value = None\n",
        "\n",
        "@functions_framework.cloud_event\n",
        "def extract_index(cloud_event):\n",
        "    index_value = None\n",
        "\n",
        "    # Define the function to process Pub/Sub messages\n",
        "    def process_message(message):\n",
        "        nonlocal index_value\n",
        "        # Extract the index from the Pub/Sub message\n",
        "        index = int(message.data.decode(\"utf-8\"))\n",
        "\n",
        "        # Save the index value\n",
        "        index_value = index\n",
        "\n",
        "        # Acknowledge the message\n",
        "        message.ack()\n",
        "\n",
        "    # Subscribe to the Pub/Sub topic\n",
        "    subscriber = pubsub_client.subscribe(subscription_name, callback=process_message)\n",
        "\n",
        "    # Wait for messages\n",
        "    subscriber.result()  # This will wait indefinitely for new messages\n",
        "\n",
        "    # Return True if index is not None\n",
        "    return index_value, index_value is not None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVzd9GEEiWaW"
      },
      "outputs": [],
      "source": [
        "# Call the function\n",
        "result, pub_sub_condition = extract_index(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb36HCYbOErb"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "from io import BytesIO\n",
        "\n",
        "def check_data_skewness(gcs_bucket, file_path):\n",
        "    # Initialize GCS client\n",
        "    client = storage.Client()\n",
        "\n",
        "    # Get bucket and blob\n",
        "    bucket = client.get_bucket(gcs_bucket)\n",
        "    blob = bucket.blob(file_path)\n",
        "\n",
        "    # Download CSV file from GCS\n",
        "    csv_data = blob.download_as_string()\n",
        "\n",
        "    # Read CSV data into pandas DataFrame\n",
        "    df = pd.read_csv(BytesIO(csv_data))\n",
        "\n",
        "    # Check if 'price_change_percentage' column exists\n",
        "    if 'price_change_percentage' not in df.columns:\n",
        "        return \"Error: 'price_change_percentage' column not found in the dataset.\"\n",
        "\n",
        "    # Calculate skewness of 'price_change_percentage' column\n",
        "    skewness = df['price_change_percentage'].skew()\n",
        "\n",
        "    # Interpret skewness\n",
        "    if -0.5 <= skewness <= 0.5:\n",
        "        interpretation = \"Fairly symmetrical distribution\"\n",
        "        return False\n",
        "    elif -1 <= skewness < -0.5 or 0.5 < skewness <= 1:\n",
        "        interpretation = \"Moderately skewed distribution\"\n",
        "        return False\n",
        "    elif skewness < -1 or skewness > 1:\n",
        "        interpretation = \"Highly skewed distribution\"\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1zkU7L4U3Ww"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def calculate_drift_threshold(gcs_bucket, file_index):\n",
        "    \"\"\"\n",
        "    Calculate the drift threshold based on the standard deviation of 'price_change_percentage' values\n",
        "    in the CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        gcs_bucket (str): Name of the Google Cloud Storage bucket.\n",
        "        file_index (int): Index to split the old and new data.\n",
        "\n",
        "    Returns:\n",
        "        float: Drift threshold.\n",
        "    \"\"\"\n",
        "    # Initialize GCS client\n",
        "    client = storage.Client()\n",
        "\n",
        "    # Get bucket and blob\n",
        "    bucket = client.get_bucket(gcs_bucket)\n",
        "    blob = bucket.blob(file_index)\n",
        "\n",
        "    # Download CSV file from GCS\n",
        "    csv_data = blob.download_as_string()\n",
        "\n",
        "    # Read CSV data into pandas DataFrame\n",
        "    df = pd.read_csv(BytesIO(csv_data))\n",
        "\n",
        "    # Get old data up to the specified index\n",
        "    old_data = df.loc[:file_index, 'price_change_percentage']\n",
        "\n",
        "    # Calculate standard deviation of old data\n",
        "    std_dev = np.std(old_data)\n",
        "\n",
        "    # Define drift threshold as a multiplier of the standard deviation\n",
        "    multiplier = 2\n",
        "    threshold = multiplier * std_dev\n",
        "\n",
        "    return threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_pSMSxDpZ6R"
      },
      "outputs": [],
      "source": [
        "def load_model_from_gcs(gcs_path):\n",
        "    \"\"\"\n",
        "    Load a TensorFlow SavedModel from Google Cloud Storage.\n",
        "\n",
        "    Parameters:\n",
        "        gcs_path (str): The path to the SavedModel directory in Google Cloud Storage.\n",
        "\n",
        "    Returns:\n",
        "        tf.saved_model.Saveable: The loaded TensorFlow SavedModel.\n",
        "    \"\"\"\n",
        "    # Load the model from GCS\n",
        "    model = tf.saved_model.load(gcs_path)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy5HziY-pdmV"
      },
      "outputs": [],
      "source": [
        "gcs_model_path = \"gs://bidirectional-lstm-model-data-science-projects-393906-model/pipeline_root/tpu_bidirectional_pipeline/model\"\n",
        "loaded_model = load_model_from_gcs(gcs_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZJE5jFKSKRp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzDzm6x6SWpj"
      },
      "outputs": [],
      "source": [
        "def calculate_prediction_drift(gcs_bucket, file_index, model,drift_threshold):\n",
        "    # Initialize GCS client\n",
        "    client = storage.Client()\n",
        "\n",
        "    # Get bucket and blob\n",
        "    bucket = client.get_bucket(gcs_bucket)\n",
        "    blob = bucket.blob(file_index)\n",
        "\n",
        "    # Download CSV file from GCS\n",
        "    csv_data = blob.download_as_string()\n",
        "\n",
        "    # Read CSV data into pandas DataFrame\n",
        "    df = pd.read_csv(BytesIO(csv_data))\n",
        "\n",
        "    # Check if 'price_change_percentage' column exists\n",
        "    if 'price_change_percentage' not in df.columns:\n",
        "        return \"Error: 'price_change_percentage' column not found in the dataset.\"\n",
        "\n",
        "    # Assuming 'price_change_percentage' is a feature used by the model\n",
        "    old_data = df.loc[:file_index, 'price_change_percentage']\n",
        "    new_data = df.loc[file_index:, 'price_change_percentage']\n",
        "\n",
        "    # Make predictions on new data\n",
        "    new_predictions = model.predict(new_data)\n",
        "\n",
        "    # Make predictions on old data\n",
        "    original_predictions = model.predict(old_data)\n",
        "\n",
        "    # Calculate prediction drift (e.g., mean absolute error)\n",
        "    prediction_drift = mean_absolute_error(original_predictions, new_predictions)\n",
        "\n",
        "    # Optionally, you can set a threshold to determine if the drift is significant\n",
        "\n",
        "    # Determine if prediction drift is significant\n",
        "    if prediction_drift > drift_threshold:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfBGwsVGOypM"
      },
      "outputs": [],
      "source": [
        "WORKER_POOL_SPECS = [\n",
        "    {\n",
        "        \"containerSpec\": {\n",
        "            \"args\": TRAINER_ARGS,\n",
        "            \"env\": [{\"name\": \"AIP_MODEL_DIR\", \"value\": WORKING_DIR}],\n",
        "            \"imageUri\": TRAIN_IMAGE,\n",
        "        },\n",
        "        \"replicaCount\": \"1\",\n",
        "        \"machineSpec\": {\n",
        "            \"machineType\": TRAIN_COMPUTE,\n",
        "            \"accelerator_type\": TRAIN_TPU,\n",
        "            \"accelerator_count\": TRAIN_NTPU,\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqlTzREAmFpp"
      },
      "outputs": [],
      "source": [
        "def deploy_model_to_endpoint(\n",
        "    project: str,\n",
        "    model_display_name: str,\n",
        "    serving_container_image_uri: str,\n",
        "    import_unmanaged_model_task,\n",
        "    WORKER_POOL_SPECS,\n",
        "    DEPLOY_COMPUTE,\n",
        "    DEPLOY_NGPU,\n",
        "    DEPLOY_GPU,\n",
        "):\n",
        "    # training job\n",
        "    custom_job_task = CustomTrainingJobOp(\n",
        "        display_name=\"tpu model training\",\n",
        "        worker_pool_specs=WORKER_POOL_SPECS,\n",
        "    )\n",
        "\n",
        "    # uploads the trained model to Google Cloud AI Platform\n",
        "    model_upload_op = ModelUploadOp(\n",
        "        project=project,\n",
        "        display_name=model_display_name,\n",
        "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
        "    )\n",
        "\n",
        "    # creates an endpoint in Google Cloud AI Platform for serving the model\n",
        "    endpoint_create_op = EndpointCreateOp(\n",
        "        project=project,\n",
        "        display_name=\"tpu-pipeline-created-endpoint\",\n",
        "    )\n",
        "\n",
        "    # deploys the uploaded model to the created endpoint\n",
        "    _ = ModelDeployOp(\n",
        "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
        "        model=model_upload_op.outputs[\"model\"],\n",
        "        # deployment configuration\n",
        "        deployed_model_display_name=model_display_name,\n",
        "        dedicated_resources_machine_type=DEPLOY_COMPUTE,\n",
        "        dedicated_resources_min_replica_count=1,\n",
        "        dedicated_resources_max_replica_count=1,\n",
        "        dedicated_resources_accelerator_type=DEPLOY_GPU.name,\n",
        "        dedicated_resources_accelerator_count=DEPLOY_NGPU,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSxhiGNeRod7"
      },
      "outputs": [],
      "source": [
        "import kfp.dsl as dsl\n",
        "from kfp.v2.dsl import component, Input, Output\n",
        "from google.cloud import pubsub\n",
        "\n",
        "# Initialize Google Cloud Pub/Sub client\n",
        "pubsub_client = pubsub.PublisherClient()\n",
        "\n",
        "# Define Pub/Sub topic name\n",
        "send_ct_pubsub_topic = \"projects/data-science-projects-393906/topics/publishMesssageCT\"\n",
        "\n",
        "@kfp.dsl.pipeline(name=\"train-endpoint-deploy\")\n",
        "def pipeline(\n",
        "    project: str = PROJECT_ID,\n",
        "    model_display_name: str = MODEL_DISPLAY_NAME,\n",
        "    serving_container_image_uri: str = DEPLOY_IMAGE,\n",
        "    gcs_bucket: str = GCS_BUCKET,\n",
        "    file_index: int = FILE_INDEX,\n",
        "    drift_threshold: float = DRIFT_THRESHOLD,\n",
        "    pub_sub_condition: bool = pub_sub_condition,\n",
        "    file_index: int = index,\n",
        "    file_path: str = \"CPU-23478.csv\",\n",
        "    loaded_model:\n",
        "):\n",
        "    # Define a condition to check for data skewness and prediction drift\n",
        "    with dsl.Condition(pub_sub_condition == True):\n",
        "        # Check data skewness\n",
        "        check_skewness = check_data_skewness(gcs_bucket, file_path)\n",
        "\n",
        "        # Check if data skewness is significant\n",
        "        with dsl.Condition(check_skewness == True):\n",
        "            # Publish a Pub/Sub message indicating significant data skewness\n",
        "            pubsub_message = \"Significant data skewness detected.\"\n",
        "            pubsub_client.publish(send_ct_pubsub_topic, data=pubsub_message.encode(\"utf-8\"))\n",
        "\n",
        "            # Start continuous training pipeline here\n",
        "            # Deployment operations\n",
        "                deploy_model_to_endpoint(\n",
        "                    project,\n",
        "                    model_display_name,\n",
        "                    serving_container_image_uri,\n",
        "                    import_unmanaged_model_task,\n",
        "                    WORKER_POOL_SPECS,\n",
        "                    DEPLOY_COMPUTE,\n",
        "                    DEPLOY_NGPU,\n",
        "                    DEPLOY_GPU,\n",
        "                )\n",
        "\n",
        "        # Check if data skewness is not significant\n",
        "        with dsl.Condition(check_skewness == False):\n",
        "            drift_threshold = calculate_drift_threshold(gcs_bucket, file_index)\n",
        "            drift_significant = calculate_prediction_drift(gcs_bucket, file_index, loaded_model, drift_threshold)\n",
        "\n",
        "            # Publish a Pub/Sub message if prediction drift is significant\n",
        "            with dsl.Condition(drift_significant == True):\n",
        "                # Publish a message to Pub/Sub topic\n",
        "                pubsub_message = \"Significant prediction drift detected. Starting continuous training.\"\n",
        "                pubsub_client.publish(send_ct_pubsub_topic, data=pubsub_message.encode(\"utf-8\"))\n",
        "\n",
        "                # Deployment operations\n",
        "                deploy_model_to_endpoint(\n",
        "                    project,\n",
        "                    model_display_name,\n",
        "                    serving_container_image_uri,\n",
        "                    import_unmanaged_model_task,\n",
        "                    WORKER_POOL_SPECS,\n",
        "                    DEPLOY_COMPUTE,\n",
        "                    DEPLOY_NGPU,\n",
        "                    DEPLOY_GPU,\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBLMgcu9Sbnc"
      },
      "outputs": [],
      "source": [
        "compiler.Compiler().compile(\n",
        "    pipeline_func=pipeline,\n",
        "    package_path=\"tpu_train_bidirectional_pipeline.json\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwhVIZdESycF",
        "outputId": "54918412-b062-4acc-d6a9-fcfcb8885578"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-endpoint-deploy-20240215171853?project=777232604101\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/777232604101/locations/us-central1/pipelineJobs/train-endpoint-deploy-20240215171853\n"
          ]
        }
      ],
      "source": [
        "DISPLAY_NAME = \"tpu_bidirectional_training\"\n",
        "\n",
        "# Creating Pipeline Job\n",
        "job = aip.PipelineJob(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    template_path=\"tpu_train_bidirectional_pipeline.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "job.run()\n",
        "\n",
        "# After the job is executed, the JSON template file\n",
        "! rm tpu_train_bidirectional_pipeline.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O0SUkLPCjlH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtbR38QQ6H8U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNdRaxyx6H-6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxmoNT1J6ICP"
      },
      "outputs": [],
      "source": [
        "def get_task_detail(\n",
        "    task_details: List[Dict[str, Any]], task_name: str\n",
        ") -> List[Dict[str, Any]]:\n",
        "    for task_detail in task_details:\n",
        "      if task_detail.task_name == task_name:\n",
        "          return task_detail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkQWmqnC6sJT"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = 'tpu_bidirectional_training'\n",
        "\n",
        "# Creating Pipeline Job\n",
        "job = aip.PipelineJob(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    template_path='tpu_train_bidirectional_pipeline.json'\n",
        "    pipeline_root = PIPELINE_ROOT,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d--7L4T7Odn"
      },
      "outputs": [],
      "source": [
        "# Warning: Setting this to true will delete everything in the bucket\n",
        "delete_bucket = False\n",
        "\n",
        "if delete_bucket and 'BUCKET_URI' in globals():\n",
        "  ! gsutil rm -r $BUCKET_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHLU7gYD7OsA"
      },
      "outputs": [],
      "source": [
        "from kfp.v2 import dsl\n",
        "from kfp.v2.dsl import component, Output\n",
        "\n",
        "# Define Cleanup pipeline\n",
        "@dsl.pipeline(name='cleanup-pipeline')\n",
        "def cleanup_pipeline(\n",
        "    project: str = PROJECT_ID,\n",
        "    endpoint_name: str = 'tpu-pipeline-created-endpoint',\n",
        "    model_display_name: str = MODEL_DISPLAY_NAME\n",
        "):\n",
        "    # Delete deployed model\n",
        "    delete_model_op = ModelDeleteOp(\n",
        "        project=project,\n",
        "        model_display_name= model_display_name,\n",
        "        delete_contents=True, # Optional: Delete Model Contents\n",
        "    )\n",
        "\n",
        "    # Delete endpoint\n",
        "    delete_endpoint_op = EndpointDleleteOp(\n",
        "        project=project,\n",
        "        endpoint_name=endpoint_name,\n",
        "    )\n",
        "\n",
        "    # Optional: Define pipeline outputs\n",
        "    # output relevant information or success indicators\n",
        "    cleanup_completed = Output(bool, description='Cleanup completed successfully.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg1GtllrA3Xf"
      },
      "outputs": [],
      "source": [
        "# Define pipeline\n",
        "@kfp.dsl.pipeline(name='train-endpoint-deploy')\n",
        "def pipeline(\n",
        "    project: str = PROJECT_ID,\n",
        "    model_deploy_name: str = MODEL_DISPLAY_NAME,\n",
        "    serving_container_image_uri: str = DEPLOY_IMAGE\n",
        "):\n",
        "\n",
        "    custom_job_task = CustomTrainingJobOp(\n",
        "        display_name='tpu model training',\n",
        "        woker_pool_specs=WORKER_POOL_SPECS,\n",
        "    )\n",
        "\n",
        "    import_unmanaged_model_task = importer_node.importer(\n",
        "        artifact_uri=WORKING_DIR,\n",
        "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
        "        metadata={\n",
        "            'containerSpec': {\n",
        "                'imageUri': serving_container_image_uri\n",
        "            },\n",
        "        },\n",
        "    ).after(custome_job_task)\n",
        "\n",
        "    model_upload_op = ModelUploadOp(\n",
        "        project=project,\n",
        "        display_name = model_display_name,\n",
        "        unmanaged_container_model=import_unmanaged_model_task_outputs['artifacts'],\n",
        "    )\n",
        "\n",
        "    endpoint_create_op = EndpointCreateOp(\n",
        "        project=project,\n",
        "        display_name='tpu-pipeline-created-endpoint'\n",
        "    )\n",
        "\n",
        "    _ = ModelDeployOp(\n",
        "        endpoint = endpoint_create_op.outputs['endpoint'],\n",
        "        model = model_upload_op.outputs['model'],\n",
        "        deployed_model_display_name=model_display_name,\n",
        "        dedicated_resources_machine_type=DEPLOY_COMPUTE,\n",
        "        dedicated_resources_min_replica_count=1,\n",
        "        dedicated_resources_max_replica_count=1,\n",
        "        dedicated_resources_accelerator_type+DEPLOY_GPU.name,\n",
        "        dedicated_resources_accelerator_aount=DEPLOY_NGPU,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCaGhIXiQk_5"
      },
      "outputs": [],
      "source": [
        "kubect1 apply -k 'github.com/kubeflow/katib.git/manifest/'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
